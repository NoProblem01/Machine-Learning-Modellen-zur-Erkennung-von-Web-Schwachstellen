{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f976b0",
   "metadata": {},
   "source": [
    "# Code zu der Arbeit:\n",
    "# \"Comparitve Study von Machine Learning Modellen zur Erkennung von Web Schwachstellen\"\n",
    "## von Nils Pudenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install kaggle scikit-learn xgboost catboost tabpfn pandas numpy matplotlib seaborn -q\n",
    "%pip install --quiet scikit-learn xgboost catboost tabpfn chardet\n",
    "%pip install -U scikit-learn\n",
    "# in deiner (Conda/venv) Umgebung\n",
    "%pip install --upgrade \"torch==2.*\" --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install --upgrade xgboost catboost scikit-learn pandas scipy tabpfn\n",
    "##wenn es komplikationen mit torch gibt, deiinstallieren und neu installieren\n",
    "%pip uninstall torch\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu121 --upgrade\n",
    "%pip install openpyxl\n",
    "%pip install XlsxWriter\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a79bf",
   "metadata": {},
   "source": [
    "## Check ob GPU verwendet werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041fa82",
   "metadata": {},
   "source": [
    "## Dowload Kaggle Datasets\n",
    "Requires Kaggle API credentials ('~/.kaggle/kaggle.json') für API-Token, um zugriff auf die Datenbanken über das Kaggle Konto zu bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dowload der Datasets von Kaggle, Output =1 erfoglgreich, Output = 0 fehlerhaft\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/sql-injection-dataset -p data --unzip --quiet\")\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning -p data --unzip --quiet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_download(dataset, path=\"data\", unzip=True):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate() #nutzt ~/.kaggle/kaggle.json für Authentifizierung oder Environment-Variablen\n",
    "    api.dataset_download_files(dataset, path=path, unzip=unzip)\n",
    "    print(f\"Downloaded {dataset}\")\n",
    "\n",
    "kaggle_download(\"syedsaqlainhussain/sql-injection-dataset\")#, path=DATA_DIR, unzip=True)\n",
    "kaggle_download(\"syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\")#, path=DATA_DIR, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77356268",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = pd.read_csv(\"data/SQLiV3.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "xss_df = pd.read_csv(\"data/XSS_dataset.csv\", encoding=\"utf-8\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa905b",
   "metadata": {},
   "source": [
    "## Infos über den Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7873ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalten ansehen\n",
    "print(sql_df.columns.tolist())\n",
    "\n",
    "# Typische Index-/Hilfsspalten loswerden\n",
    "sql_df = sql_df.loc[:, ~sql_df.columns.str.contains(r\"^Unnamed|^index$\", case=False)]\n",
    "\n",
    "# Auf die Kernspalten reduzieren (falls etwas anderes drin ist)\n",
    "sql_df = sql_df[[\"Sentence\", \"Label\"]].copy()\n",
    "\n",
    "# Optional: Duplikate auf Satzebene entfernen (falls noch nicht passiert)\n",
    "sql_df = sql_df.drop_duplicates(subset=[\"Sentence\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"clean shape:\", sql_df.shape)  # Erwartung: (30873, 2)\n",
    "print(sql_df[\"Label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in {\"SQL\": sql_df, \"XSS\": xss_df}.items():\n",
    "    print(f\"{name} dataset shape: {df.shape}\")\n",
    "    display(df.head(20))\n",
    "    display(df.describe())\n",
    "    display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dc485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(df, label_col=\"Label\"):\n",
    "    \"\"\"Zeigt die Klassenverteilung in einem DataFrame.\"\"\"\n",
    "    counts = df[label_col].value_counts(normalize=True)\n",
    "    print(\"Klassenverteilung:\")\n",
    "    print(counts)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Klassenverteilung\")\n",
    "    plt.xlabel(label_col)\n",
    "    plt.ylabel(\"Anteil\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "print(\"SQL Distribution:\")\n",
    "print(class_distribution(sql_df, \"Label\"))\n",
    "\n",
    "print(\"XSS Distribution:\")\n",
    "print(class_distribution(xss_df, \"Label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61750e3b",
   "metadata": {},
   "source": [
    "## Basic Cleaning\n",
    "* Drop Duplicate rows\n",
    "* Handle missing values (simple fill-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (sql_df, xss_df):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_xy(df: pd.DataFrame,\n",
    "                  label_candidates=(\"label\", \"class\", \"target\"),\n",
    "                  label_map=None):\n",
    "    if label_map is None:\n",
    "        label_map = {\n",
    "            \"0\": \"0\", \"1\": \"1\",\n",
    "            \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "            \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\",\n",
    "            \"sql-injection\": \"1\", \"xss\": \"1\"\n",
    "        }\n",
    "\n",
    "    # Zielspalte finden (im *übergebenen* df!)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    target_col = next((cols_lower[c] for c in label_candidates if c in cols_lower), None)\n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"Keine Label-Spalte gefunden. Kandidaten: {label_candidates}\")\n",
    "\n",
    "    # Labels normieren -> nur 0/1 behalten\n",
    "    y_str = df[target_col].astype(str).str.strip().str.lower()\n",
    "    y_map = y_str.map(label_map)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "\n",
    "    # Rohtext aus allen Nicht-Label-Spalten zusammenbauen\n",
    "    feat_cols = [c for c in df.columns if c != target_col]\n",
    "    X_raw = df.loc[mask, feat_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    return X_raw, y, target_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8014a08",
   "metadata": {},
   "source": [
    "# Globale Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa731aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deterministische Ausgabe generieren, um die Reproduzierbarkeit zu gewährleisten\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()  # für TabPFN & XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e741890",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_metrics(y_true, y_pred) -> dict:\n",
    "    \"\"\"Präzision/Recall/F1 & Raten (FPR/FNR) für binäre Klassifikation.\"\"\"\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    p  = precision_score(y_true, y_pred)\n",
    "    r  = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "    return dict(Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, name, use_batches=False, batch_size=256) -> dict: #Dictionary für Evaluierungsmetriken\n",
    "    \"\"\"Zeitmessung + Vorhersage (optional in Batches) + Metriken.\"\"\"\n",
    "    print(f\"→ Evaluate {name} on X_test={getattr(X_test,'shape',None)}\")\n",
    "    t0 = time.perf_counter()\n",
    "    if use_batches:\n",
    "        y_pred = predict_in_batches(model, X_test, batch_size=batch_size, verbose=True)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    pred_s = time.perf_counter() - t0\n",
    "    m = binary_metrics(y_test, y_pred)\n",
    "    res = dict(Model=name, Pred_s=pred_s, **m)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels=None, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Zeigt eine Konfusionsmatrix an.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment='center',\n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30bc87",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246ba34",
   "metadata": {},
   "source": [
    "### Batchweise Vorhersage für TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, X, batch_size=256, verbose=False):\n",
    "    \"\"\"Vorhersage in Batches (schont RAM/VRAM; wichtig für TabPFN).\"\"\"\n",
    "    n = X.shape[0]\n",
    "    out = []\n",
    "    total = math.ceil(n / batch_size)\n",
    "    for b, i in enumerate(range(0, n, batch_size), start=1):\n",
    "        j = min(i + batch_size, n)\n",
    "        t1 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            out.append(model.predict(X[i:j]))\n",
    "        dt = time.perf_counter() - t1\n",
    "        if verbose:\n",
    "            print(f\"   [predict] batch {b:>3}/{total} ({j-i} rows) in {dt:.2f}s\")\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "    return np.concatenate(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85297bac",
   "metadata": {},
   "source": [
    "### Label bereinigung der Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(df, label_col=\"Label\", text_cols=(\"Sentence\",)):\n",
    "    \"\"\"Bringt Labels robust auf {0,1} und gibt (X_raw, y) zurück.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Labels in {0,1} umwandeln\n",
    "    y_raw = df[label_col].astype(str).str.strip().str.lower()\n",
    "    map01 = {\n",
    "        \"0\": \"0\", \"1\": \"1\", \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "        \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\", \"sql-injection\": \"1\", \"xss\": \"1\"\n",
    "    }\n",
    "    y_map = y_raw.map(map01)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "    \n",
    "    # Explizit nur angegebene Textspalten verwenden!\n",
    "    X_raw = df.loc[mask, text_cols].astype(str).agg(\" \".join, axis=1)\n",
    "    \n",
    "    return X_raw, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd2d56",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acf949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls clean_labels(X) bereits (Rohtext, y) zurückgibt:\n",
    "X_txt_sql, y_sql = clean_labels(sql_df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "X_train_sql, X_test_sql, y_train_sql, y_test_sql = train_test_split(\n",
    "    X_txt_sql, y_sql, test_size=0.2, stratify=y_sql, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_txt_xss, y_xss = clean_labels(xss_df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "X_train_xss, X_test_xss, y_train_xss, y_test_xss = train_test_split(\n",
    "    X_txt_xss, y_xss, test_size=0.2, stratify=y_xss, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(y, title=\"Label-Verteilung\"):\n",
    "    \"\"\"Zeigt die Label-Verteilung in einem Balkendiagramm.\"\"\"\n",
    "    counts = pd.Series(y).value_counts(normalize=True)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Label (0=benign, 1=attack)\")\n",
    "    plt.ylabel(\"Anzahl\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(y_train_sql, \"SQL Train Label-Verteilung\")\n",
    "plot_class_distribution(y_test_sql, \"SQL Test Label-Verteilung\")\n",
    "\n",
    "plot_class_distribution(y_train_xss, \"XSS Train Label-Verteilung\")\n",
    "plot_class_distribution(y_test_xss, \"XSS Test Label-Verteilung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59795bab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bf220ae",
   "metadata": {},
   "source": [
    "# Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f88842",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(256,128), activation=\"relu\",\n",
    "                         early_stopping=True, n_iter_no_change=5, max_iter=200,\n",
    "                         random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.9, colsample_bytree=0.8,\n",
    "        tree_method=\"hist\", device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=400, depth=8, learning_rate=0.1,\n",
    "        loss_function=\"Logloss\", random_seed=RANDOM_STATE, verbose=False,\n",
    "        task_type=\"CPU\"   # stabil über Pool + Sparse; bei GPU: task_type=\"GPU\" und ggf. Dense verwenden\n",
    "    ),\n",
    "    \"TabPFN\": TabPFNClassifier(\n",
    "        device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        ignore_pretraining_limits=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f941b9d",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696592bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDF-IDF Einstellungen\n",
    "VEC_ARGS = dict(analyzer=\"char\", ngram_range=(3,5), min_df=3, max_features=50_000, sublinear_tf=True, lowercase=False, dtype=np.float32)\n",
    "\n",
    "\n",
    "TABPFN_MAX_SAMPLES = 4000     # starte konservativ; später 6000/8000 testen\n",
    "TABPFN_N_COMPONENTS = 150     # <=500, kleiner = schneller/ram-sparender\n",
    "TABPFN_BATCH = 128            # Batch für Predict; 128/64 bei RAM-Engpässen\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a076ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline je Datensatz (TF-IDF; TabPFN: SVD + batched predict)\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from catboost import Pool\n",
    "import numpy as np, time, torch\n",
    "\n",
    "\n",
    "splits = {\n",
    "    \"SQL\": (X_train_sql, X_test_sql, y_train_sql, y_test_sql),\n",
    "    \"XSS\": (X_train_xss, X_test_xss, y_train_xss, y_test_xss),\n",
    "}\n",
    "\n",
    "for ds_name, (X_train_txt, X_test_txt, y_train, y_test) in splits.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"DATASET: {ds_name} | train={len(y_train)} test={len(y_test)} (pos_rate_train={np.mean(y_train):.3f})\")\n",
    "\n",
    "    #TF-IDF nur auf Train fitten (kein Leakage)\n",
    "    vec = TfidfVectorizer(**VEC_ARGS)\n",
    "    t0 = time.perf_counter()\n",
    "    X_train_vec = vec.fit_transform(X_train_txt)\n",
    "    #Feature anzeigen\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    print(f\"TF-IDF Features: {len(feature_names)}\")\n",
    "    print(\"Beispiel-Feature:\", feature_names[:10])  # Zeigt das\n",
    "    X_test_vec  = vec.transform(X_test_txt)\n",
    "    print(f\"[VEC] TF-IDF train={X_train_vec.shape}, test={X_test_vec.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "    # Füge K-fold Cross Validation auf den Trainingsdaten hinzu:\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "    print(f\"\\nPerforming {cv.get_n_splits()}‑Fold CV on the training data:\\n\")\n",
    "\n",
    "    # Beispielweise mit F1-Score – beachte, dass manche Modelle (z.B. CatBoost, TabPFN) evtl. spezielle Datenformate erwarten:\n",
    "    for name, base_model in models.items():\n",
    "        model = clone(base_model)\n",
    "        # Falls das Modell direkt den Sparse Input akzeptiert, kannst du CV nutzen:\n",
    "        if name not in [\"CatBoost\", \"TabPFN\"]:\n",
    "            scores = cross_val_score(model, X_train_vec, y_train, cv=cv, scoring=\"f1\")\n",
    "            print(f\"[{name}] CV F1 Score: {scores.mean():.4f} (± {scores.std():.4f})\")\n",
    "        else:\n",
    "            print(f\"[{name}] CV abgebrochen, da spezielles Preprocessing erforderlich ist.\")\n",
    "\n",
    "    #Modelle trainieren/evaluieren (jeweils frischen Klon verwenden)\n",
    "    for name, base_model in models.items():\n",
    "        model = clone(base_model)\n",
    "        print(\"-\"*50 + f\"\\nMODEL: {name} on {ds_name}\")\n",
    "\n",
    "        if name == \"CatBoost\":\n",
    "            # CatBoost: Pool nutzen (sparse ok)\n",
    "            pool_train = Pool(X_train_vec, y_train)\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(pool_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "            # Test direkt auf X_test_vec (CatBoost kann CSR); alternativ: Pool(X_test_vec, y_test)\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        elif name == \"TabPFN\":\n",
    "            # TabPFN: ggf. stratifiziertes Subset (Limits), danach SVD -> dichte float32\n",
    "            X_tab, y_tab = X_train_vec, y_train\n",
    "            if X_train_vec.shape[0] > TABPFN_MAX_SAMPLES:\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, train_size=TABPFN_MAX_SAMPLES, random_state=RANDOM_STATE)\n",
    "                idx, _ = next(sss.split(np.zeros(len(y_train)), y_train))\n",
    "                X_tab = X_train_vec[idx]\n",
    "                y_tab = np.asarray(y_train)[idx]\n",
    "            print(f\"[{name}] train subset: {X_tab.shape[0]} rows\")\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            svd = TruncatedSVD(n_components=TABPFN_N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "            X_train_svd = svd.fit_transform(X_tab).astype(\"float32\", copy=False)\n",
    "            X_test_svd  = svd.transform(X_test_vec).astype(\"float32\", copy=False)\n",
    "            print(f\"[{name}] SVD train={X_train_svd.shape}, test={X_test_svd.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "            # Gerät einstellen & fitten\n",
    "            if hasattr(model, \"set_params\"):\n",
    "                model.set_params(device=(\"cuda\" if USE_CUDA else \"cpu\"), ignore_pretraining_limits=True)\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_svd, y_tab)\n",
    "            if USE_CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s (device={'cuda' if USE_CUDA else 'cpu'})\")\n",
    "\n",
    "            # Evaluieren (wenn deine evaluate_model batched predict unterstützt, ansonsten normal)\n",
    "            try:\n",
    "                res = evaluate_model(model, X_test_svd, y_test, f\"{name}-{ds_name}\",\n",
    "                                     use_batches=True, batch_size=TABPFN_BATCH)\n",
    "            except TypeError:\n",
    "                res = evaluate_model(model, X_test_svd, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        else:\n",
    "            # RF / MLP / XGBoost: direkt auf sparse TF-IDF\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1bccb",
   "metadata": {},
   "source": [
    "# Ergebnisse für Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnis-Tabelle ausgeben\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results.sort_values([\"Model\"]).reset_index(drop=True))\n",
    "df_results.to_csv(\"results_all.csv\", index=False)\n",
    "with pd.ExcelWriter(\"results_all.xlsx\") as w:\n",
    "    df_results.to_excel(w, sheet_name=\"All\", index=False)\n",
    "print(\" Ergebnisse gespeichert: results_all.csv / results_all.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e6a3",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning & Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad6773",
   "metadata": {},
   "source": [
    "## Feature und Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Builder (schnell & schlank)\n",
    "\n",
    "def make_tfidf(max_features=20_000):\n",
    "    return TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=(3,5),\n",
    "        min_df=3, max_features=max_features,\n",
    "        sublinear_tf=True, lowercase=False,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "# Cache, damit TF-IDF/SVD nicht in jedem Kandidaten neu gelernt werden müssen\n",
    "memory = Memory(location=\"skcache\", verbose=0)\n",
    "\n",
    "\n",
    "#schlanke Suchräume\n",
    "def pos_weight(y):\n",
    "    pos = max(1, int(np.sum(y))); neg = max(1, int(len(y)-pos))\n",
    "    return neg / pos\n",
    "\n",
    "def grids(y):\n",
    "    return {\n",
    "        \"RF\": {\n",
    "            \"svd__n_components\": [120, 150, 200],\n",
    "            \"clf__n_estimators\": [300, 600],\n",
    "            \"clf__max_depth\": [None, 20, 40],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__max_features\": [\"sqrt\", None],\n",
    "        },\n",
    "        \"MLP\": {\n",
    "            \"svd__n_components\": [120, 150, 200],\n",
    "            \"clf__hidden_layer_sizes\": [(256,128), (512,256)],\n",
    "            \"clf__alpha\": np.logspace(-5, -3, 3),\n",
    "            \"clf__learning_rate_init\": [1e-4, 5e-4, 1e-3],\n",
    "            \"clf__batch_size\": [64, 128],\n",
    "        },\n",
    "        \"XGB\": {\n",
    "            \"tfidf__max_features\": [15_000, 20_000],\n",
    "            \"clf__n_estimators\": [300, 500],\n",
    "            \"clf__max_depth\": [4, 6, 8],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__subsample\": [0.7, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 1.0],\n",
    "            \"clf__reg_lambda\": [0, 5],\n",
    "            \"clf__gamma\": [0, 1],\n",
    "            \"clf__scale_pos_weight\": [pos_weight(y)],\n",
    "        },\n",
    "        \"CAT\": {\n",
    "            \"tfidf__max_features\": [15_000, 20_000],\n",
    "            \"clf__iterations\": [400, 800],\n",
    "            \"clf__depth\": [6, 8],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__l2_leaf_reg\": [3, 5],\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b9d1d",
   "metadata": {},
   "source": [
    "## HPO-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75dfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_df, title=\"Modellevaluation\", metric=\"F1\"):\n",
    "    \"\"\"Zeigt die Ergebnisse als Balkendiagramm an.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    results_df.sort_values(by=metric, ascending=False).plot(\n",
    "        x=\"Model\", y=metric, kind=\"bar\", legend=False, color=\"skyblue\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Modelle\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpo_subset(X, y, n=8000):\n",
    "    \"\"\"Erzeugt ein stratifiziertes Subset für HPO.\"\"\"\n",
    "    if len(y) <= n:\n",
    "        return X, y\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=n, random_state=RANDOM_STATE)\n",
    "    idx, _ = next(sss.split(np.zeros(len(y)), y))\n",
    "    if hasattr(X, \"iloc\"):  # DataFrame oder Series\n",
    "        return X.iloc[idx], np.asarray(y)[idx]\n",
    "    else:\n",
    "        return X[idx], np.asarray(y)[idx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6efee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines\n",
    "def pipe_rf_svd(max_features=20_000, n_comp=150):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"svd\", TruncatedSVD(n_components=n_comp, random_state=RANDOM_STATE)),\n",
    "        (\"clf\", RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_mlp_svd(max_features=20_000, n_comp=150):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"svd\", TruncatedSVD(n_components=n_comp, random_state=RANDOM_STATE)),\n",
    "        (\"scaler\", StandardScaler(with_mean=True)),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            early_stopping=True, n_iter_no_change=8, max_iter=200,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_xgb_sparse(max_features=20_000):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "            n_estimators=400,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1  # Parallelisierung über CV, nicht im Estimator\n",
    "        ))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_cat_sparse(max_features=20_000):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"clf\", CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            verbose=False,\n",
    "            random_seed=RANDOM_STATE\n",
    "        ))\n",
    "    ], memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99717f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemeinsames CV-Objekt für faire Vergleiche\n",
    "cv_shared = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Hilfsfunktion für Hyperparameter-Suche\n",
    "def make_search(estimator, param_distributions, y_train, use_halving=True, cv=None):\n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scorer = make_scorer(f1_score)\n",
    "    if use_halving:\n",
    "        n = len(y_train)\n",
    "        min_res = max(200, int(0.15 * n))\n",
    "        return HalvingRandomSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_distributions,\n",
    "            resource=\"n_samples\",\n",
    "            min_resources=min_res,\n",
    "            max_resources=\"auto\",\n",
    "            factor=3,\n",
    "            scoring=scorer,\n",
    "            cv=cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=2,\n",
    "            verbose=2\n",
    "        )\n",
    "    else:\n",
    "        return RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=10,\n",
    "            scoring=scorer,\n",
    "            cv=cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=2,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "# Training + Suche + Refit + Threshold\n",
    "def tune_one(name, X_train_txt, y_train, cv):\n",
    "    if name == \"RF\":   pipe = pipe_rf_svd()\n",
    "    elif name == \"MLP\": pipe = pipe_mlp_svd()\n",
    "    elif name == \"XGB\": pipe = pipe_xgb_sparse()\n",
    "    elif name == \"CAT\": pipe = pipe_cat_sparse()\n",
    "    else: raise ValueError(name)\n",
    "\n",
    "    # Teilmenge für HPO\n",
    "    X_hpo, y_hpo = get_hpo_subset(X_train_txt, y_train, n=8000)\n",
    "    space = grids(y_hpo)[name]\n",
    "    search = make_search(pipe, space, y_hpo, use_halving=True, cv=cv)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    search.fit(X_hpo, y_hpo)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[{name}] best F1 (CV): {search.best_score_:.4f} in {dt/60:.1f} min\")\n",
    "    print(f\"[{name}] best params:\", search.best_params_)\n",
    "\n",
    "    best = clone(search.best_estimator_)\n",
    "    best.fit(X_train_txt, y_train)\n",
    "    return best, search.best_params_, search.best_score_, dt\n",
    "\n",
    "# Threshold-Optimierung + Testauswertung\n",
    "def eval_on_test(name, est, X_train_txt, y_train, X_test_txt, y_test, tune_threshold=True):\n",
    "    thr = 0.5\n",
    "    if tune_threshold and hasattr(est, \"predict_proba\"):\n",
    "        X_tr2, X_val, y_tr2, y_val = train_test_split(X_train_txt, y_train, test_size=0.15, stratify=y_train, random_state=RANDOM_STATE)\n",
    "        est_for_thr = est\n",
    "        est_for_thr.fit(X_tr2, y_tr2)\n",
    "        p = est_for_thr.predict_proba(X_val)[:, 1]\n",
    "        grid = np.linspace(0.1, 0.9, 81)\n",
    "        thr = float(grid[int(np.argmax([f1_score(y_val, (p >= t).astype(int)) for t in grid]))])\n",
    "\n",
    "    y_pred = (est.predict_proba(X_test_txt)[:,1] >= thr).astype(int) if hasattr(est, \"predict_proba\") else est.predict(X_test_txt)\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=[0, 1], title=f\"Confusion Matrix {name} (Thr={thr:.2f})\")\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "    return dict(Model=name, Thr=thr, Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n",
    "\n",
    "# Ausführung mit übergebenem Split\n",
    "def run_fast_from_split(X_train_txt, X_test_txt, y_train, y_test, ds_name, cv):\n",
    "    results = []\n",
    "    for short, label in [(\"RF\", \"RandomForest\"), (\"MLP\", \"MLP\"), (\"XGB\", \"XGBoost\"), (\"CAT\", \"CatBoost\")]:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"TUNING: {label} ({ds_name})\")\n",
    "        best_est, params, best_cv, dt = tune_one(short, X_train_txt, y_train, cv=cv)\n",
    "        res = eval_on_test(f\"{label}-{ds_name}\", best_est, X_train_txt, y_train, X_test_txt, y_test)\n",
    "        print(\"TEST:\", res)\n",
    "        results.append(res)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Anwendung auf beide Datensätze\n",
    "sql_res = run_fast_from_split(X_train_sql, X_test_sql, y_train_sql, y_test_sql, ds_name=\"SQL\", cv=cv_shared)\n",
    "xss_res = run_fast_from_split(X_train_xss, X_test_xss, y_train_xss, y_test_xss, ds_name=\"XSS\", cv=cv_shared)\n",
    "\n",
    "display(sql_res)\n",
    "display(xss_res)\n",
    "plot_results(sql_res, title=\"SQL Modellevaluation\", metric=\"F1\")\n",
    "plot_results(xss_res, title=\"XSS Modellevaluation\", metric=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results and tag with dataset names\n",
    "sql_res[\"Dataset\"] = \"SQL\"\n",
    "xss_res[\"Dataset\"] = \"XSS\"\n",
    "\n",
    "df_all = pd.concat([sql_res, xss_res], ignore_index=True)\n",
    "\n",
    "# Anzeigen\n",
    "display(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62c2e3",
   "metadata": {},
   "source": [
    "# LLM-Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip show openai\n",
    "#%pip install openai==0.28\n",
    "#%pip install --upgrade openai\n",
    "#%pip install python-dotenv\n",
    "#%pip install mistralai tenacity\n",
    "#%pip install mistralai \n",
    "#%pip show mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be077380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_div(num, den):\n",
    "    return float(num/den) if den else 0.0\n",
    "\n",
    "def compute_binary_metrics(y_true, y_pred):\n",
    "    \"\"\"Berechnet binäre Klassifikationsmetriken für positive Klasse (1 = malicious).\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "\n",
    "    TP = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    TN = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    FP = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    FN = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "\n",
    "    precision = _safe_div(TP, TP + FP)\n",
    "    recall    = _safe_div(TP, TP + FN)\n",
    "    f1        = _safe_div(2*precision*recall, precision + recall) if (precision+recall) else 0.0\n",
    "    fpr       = _safe_div(FP, FP + TN)\n",
    "    specificity = _safe_div(TN, TN + FP)\n",
    "    accuracy  = _safe_div(TP + TN, TP + TN + FP + FN)\n",
    "\n",
    "    return {\n",
    "        \"N_valid\": int(len(y_true)),\n",
    "        \"Support_pos(1)\": int(np.sum(y_true == 1)),\n",
    "        \"Support_neg(0)\": int(np.sum(y_true == 0)),\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"FPR\": fpr,\n",
    "        \"Specificity\": specificity,\n",
    "    }\n",
    "\n",
    "def confusion_matrix_df(y_true, y_pred):\n",
    "    \"\"\"Erstellt eine Pandas-Confusion-Matrix.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    cm = pd.crosstab(pd.Series(y_true, name=\"True\"),\n",
    "                     pd.Series(y_pred, name=\"Pred\"),\n",
    "                     dropna=False)\n",
    "    for cls in [0,1]:\n",
    "        if cls not in cm.index: cm.loc[cls] = 0\n",
    "        if cls not in cm.columns: cm[cls] = 0\n",
    "    return cm.sort_index().sort_index(axis=1)\n",
    "\n",
    "def metrics_for_dataset(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Wrapper: zieht y_true / y_pred aus df und berechnet Metriken + Confusion Matrix.\"\"\"\n",
    "    valid = df.dropna(subset=[\"Pred\"]).copy()\n",
    "    if valid.empty:\n",
    "        return name, pd.DataFrame([{\"Dataset\": name, \"N_valid\": 0}]), pd.DataFrame()\n",
    "    y_true = valid[\"True\"].astype(int).values\n",
    "    y_pred = valid[\"Pred\"].astype(int).values\n",
    "    m = compute_binary_metrics(y_true, y_pred)\n",
    "    m[\"Dataset\"] = name\n",
    "    cm = confusion_matrix_df(y_true, y_pred)\n",
    "    return name, pd.DataFrame([m]), cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe64288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_sql, metrics_sql_df, cm_sql = metrics_for_dataset(df_llm_sql, \"SQL\")\n",
    "name_xss, metrics_xss_df, cm_xss = metrics_for_dataset(df_llm_xss, \"XSS\")\n",
    "\n",
    "print(\"=== Metrics: SQL ===\")\n",
    "display(metrics_sql_df.style.format({\n",
    "    \"Accuracy\":\"{:.4f}\", \"Precision\":\"{:.4f}\", \"Recall\":\"{:.4f}\", \"F1\":\"{:.4f}\",\n",
    "    \"FPR\":\"{:.4f}\", \"Specificity\":\"{:.4f}\"\n",
    "}))\n",
    "print(\"Confusion Matrix (SQL):\")\n",
    "display(cm_sql)\n",
    "\n",
    "print(\"\\n=== Metrics: XSS ===\")\n",
    "display(metrics_xss_df.style.format({\n",
    "    \"Accuracy\":\"{:.4f}\", \"Precision\":\"{:.4f}\", \"Recall\":\"{:.4f}\", \"F1\":\"{:.4f}\",\n",
    "    \"FPR\":\"{:.4f}\", \"Specificity\":\"{:.4f}\"\n",
    "}))\n",
    "print(\"Confusion Matrix (XSS):\")\n",
    "display(cm_xss)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
